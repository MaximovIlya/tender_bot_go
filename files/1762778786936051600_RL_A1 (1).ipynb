{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Dynamic Programming - The Gambler's Problem\n",
        "\n",
        "**Objective:** Implement and compare Value Iteration and Policy Iteration to solve the Gambler's Problem as described in Chapter 4 of Sutton and Barto's Reinforcement Learning: An Introduction."
      ],
      "metadata": {
        "id": "assignment_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Gambler's Problem\n",
        "\n",
        "A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many dollars as he has staked on that flip, if it is tails, he loses his stake. The game ends when the gambler wins by reaching his goal of 100 USD, or loses by running out of money (capital is 0 USD).\n",
        "\n",
        "This problem can be formulated as an undiscounted, episodic, finite MDP:\n",
        "\n",
        "-   **States ($s$):** The gambler's capital, $s \\in \\{1, 2, \\dots, 99\\}$. The states $s = 0$ and $s = 100$ are terminal states.\n",
        "-   **Actions ($a$):** The integer amount to stake, $a \\in \\{1, 2, \\dots, \\min(s, 100-s)\\}$.\n",
        "-   **Reward ($R$):** The reward is +1 on the transition to the winning state ($100), and 0 on all other transitions.\n",
        "\n",
        "-   **Transitions ($p(s'|s,a)$):** From a state $s$, after taking action $a$, the gambler's capital will become $s+a$ with probability $p_h$ (the probability of heads) and $s-a$ with probability $1-p_h$.\n",
        "-   **Discount Factor ($\\gamma$):** 1 (undiscounted).\n",
        "\n",
        "Your objective is to find an optimal policy, $\\pi^*(s)$, which maps each capital level to a stake amount, that maximizes the probability of reaching the goal of 100 USD. The value of a state $s$, $V(s)$, is then the probability of winning starting with capital $s$."
      ],
      "metadata": {
        "id": "problem_def"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Value Iteration\n",
        "\n",
        "Value Iteration finds the optimal value function $V^*$ by turning the Bellman Optimality Equation into an iterative update.\n",
        "\n",
        "**General Bellman Optimality Equation:**\n",
        "$$V_{k+1}(s)  =  \\max_{a} \\sum_{s', r} p(s', r | s, a) [r + \\gamma V_k(s')]$$\n",
        "\n",
        "**For the Gambler's Problem, this becomes:**\n",
        "$$V_{k+1}(s)  =  \\max_{a \\in \\{1, ..., \\min(s, 100-s)\\}} \\Big[ p_h \\cdot (R_{s+a} + \\gamma V_k(s+a)) + (1-p_h) \\cdot (R_{s-a} + \\gamma V_k(s-a)) \\Big]$$\n",
        "Where $R_{s+a} = 1$ if $s+a = 100$, and 0 otherwise.\n",
        "\n",
        "### Your Task:\n",
        "Implement the `value_iteration` method in the `GamblersProblem` class below. You will need to:\n",
        "1. Loop until the value function converges (for example, the maximum change between sweeps, `delta`, is less than `theta`).\n",
        "2. In each sweep, loop through all states $s \\in \\{1, ..., 99\\}$.\n",
        "3. For each state, find the action that maximizes the expected future value.\n",
        "4. Update the value function `V[s]` with this maximum value.\n",
        "5. After the value function has converged, extract the optimal policy."
      ],
      "metadata": {
        "id": "vi_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GamblersProblem:\n",
        "    def __init__(self, p_h  =  0.4, gamma  =  1.0, theta  =  1e-9):\n",
        "        self.p_h  =  p_h  # Probability of heads\n",
        "        self.gamma  =  gamma\n",
        "        self.theta  =  theta\n",
        "\n",
        "        self.states  =  np.arange(101)\n",
        "        self.V  =  np.zeros(101)\n",
        "        self.V[100]  =  1.0\n",
        "\n",
        "        self.policy  =  np.zeros(101, dtype  =  int)\n",
        "\n",
        "    def value_iteration(self):\n",
        "        \"\"\"Performs the Value Iteration algorithm to find the optimal value function.\"\"\"\n",
        "        sweep  =  0\n",
        "        history  =  [self.V.copy()]\n",
        "\n",
        "        while True:\n",
        "            delta  =  0\n",
        "            # Here is your code\n",
        "            # TODO: Implement one sweep of Value Iteration\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Here end your code\n",
        "            sweep +=  1\n",
        "            history.append(self.V.copy())\n",
        "            if delta < self.theta:\n",
        "                break\n",
        "\n",
        "        print(f\"Value Iteration converged after {sweep} sweeps.\")\n",
        "\n",
        "        # Policy Extraction\n",
        "        # Here is your code\n",
        "        # TODO: Extract the optimal policy from the converged value function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Here end your code\n",
        "\n",
        "        return history, self.policy, self.V\n",
        "\n",
        "    def plot_value_function_history(self, history):\n",
        "        plt.figure(figsize = (12, 8))\n",
        "        plot_indices  =  np.unique(np.logspace(0, np.log10(len(history)-1), 5, dtype = int))\n",
        "        for i in plot_indices:\n",
        "            plt.plot(self.states[1:100], history[i][1:100], label = f'Sweep {i}')\n",
        "        plt.title('Value Function Estimates over Sweeps (Value Iteration)')\n",
        "        plt.xlabel('Capital')\n",
        "        plt.ylabel('Value (Probability of Winning)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_policy(self):\n",
        "        plt.figure(figsize = (12, 8))\n",
        "        plt.step(self.states[1:100], self.policy[1:100])\n",
        "        plt.title('Final Policy (Stake vs. Capital)')\n",
        "        plt.xlabel('Capital')\n",
        "        plt.ylabel('Stake')\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "vi_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here you run your Value Iteration\n",
        "gambler_vi  =  GamblersProblem(p_h = 0.4)\n",
        "vi_history, vi_policy, vi_values  =  gambler_vi.value_iteration()\n",
        "\n",
        "gambler_vi.plot_value_function_history(vi_history)\n",
        "gambler_vi.plot_policy()"
      ],
      "metadata": {
        "id": "vi_run"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Policy Iteration\n",
        "\n",
        "Policy Iteration alternates between two steps:\n",
        "1. **Policy Evaluation:** Given a policy $\\pi$, compute the value function $V^\\pi$. This is done by repeatedly applying the Bellman Expectation Equation until convergence.\n",
        "   $$V_{k+1}^{\\pi}(s)  =  \\sum_{s', r} p(s', r | s, \\pi(s)) [r + \\gamma V_k^{\\pi}(s')]$$\n",
        "   **For the Gambler's Problem:**\n",
        "   $$V_{k+1}^{\\pi}(s)  =  p_h \\cdot (R_{s+\\pi(s)} + \\gamma V_k^{\\pi}(s+\\pi(s))) + (1-p_h) \\cdot (R_{s-\\pi(s)} + \\gamma V_k^{\\pi}(s-\\pi(s)))$$\n",
        "\n",
        "2. **Policy Improvement:** Given the value function $V^\\pi$, improve the policy by acting greedily with respect to it.\n",
        "   $$\\pi'(s)  =  \\arg\\max_{a} \\sum_{s', r} p(s', r | s, a) [r + \\gamma V^\\pi(s')]$$\n",
        "\n",
        "The process stops when the policy is stable (for example it doesn't change after an improvement step).\n",
        "\n",
        "### Your Task:\n",
        "Implement the `policy_iteration`, `policy_evaluation`, and `policy_improvement` methods.\n",
        "1.  **`policy_evaluation`**: This will be a loop inside your main `policy_iteration` loop. It should update `self.V` for the *current* fixed policy `self.policy` until convergence.\n",
        "2.  **`policy_improvement`**: This will iterate through all states and find the greedy action with respect to the *newly evaluated* `self.V`. It should update `self.policy` and return `True` if the policy is stable, `False` otherwise."
      ],
      "metadata": {
        "id": "pi_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GamblersProblemPI(GamblersProblem):\n",
        "    def __init__(self, p_h = 0.4, gamma = 1.0, theta = 1e-9):\n",
        "        super().__init__(p_h, gamma, theta)\n",
        "        # Start with a simple policy: stake 1 dollar no matter what is the capital\n",
        "        self.policy  =  np.ones(101, dtype = int)\n",
        "        self.policy[0]  =  0\n",
        "        self.policy[100]  =  0\n",
        "\n",
        "    def policy_evaluation(self):\n",
        "        \"\"\"Performs iterative policy evaluation for the current policy.\"\"\"\n",
        "        # Here is your code\n",
        "        # TODO: Implement the policy evaluation loop.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Here end your code\n",
        "\n",
        "    def policy_improvement(self):\n",
        "        \"\"\"Improves the policy by acting greedily with respect to the value function.\"\"\"\n",
        "        policy_stable  =  True\n",
        "         # Here goes your code\n",
        "        # TODO: Implement the policy improvement step.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Here end your code\n",
        "        return policy_stable\n",
        "\n",
        "    def policy_iteration(self):\n",
        "        \"\"\"Performs the Policy Iteration algorithm.\"\"\"\n",
        "        iteration  =  0\n",
        "        policy_history  =  [self.policy.copy()]\n",
        "        while True:\n",
        "            self.policy_evaluation()\n",
        "            policy_stable  =  self.policy_improvement()\n",
        "            policy_history.append(self.policy.copy())\n",
        "            iteration +=  1\n",
        "            if policy_stable:\n",
        "                break\n",
        "\n",
        "        print(f\"Policy Iteration converged after {iteration} iterations.\")\n",
        "        return policy_history, self.policy, self.V\n",
        "\n",
        "    def plot_policy_history(self, history):\n",
        "        \"\"\"Plots the policy at different stages of iteration.\"\"\"\n",
        "        plt.figure(figsize = (12, 8))\n",
        "        plot_indices  =  np.unique(np.linspace(0, len(history)-1, 5, dtype = int))\n",
        "        for i in plot_indices:\n",
        "             plt.plot(self.states[1:100], history[i][1:100], label = f'Iteration {i}')\n",
        "        plt.title('Policy Evolution over Iterations')\n",
        "        plt.xlabel('Capital')\n",
        "        plt.ylabel('Stake')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_value_function(self):\n",
        "        \"\"\"Plots the final value function.\"\"\"\n",
        "        plt.figure(figsize = (12, 8))\n",
        "        plt.plot(self.states[1:100], self.V[1:100])\n",
        "        plt.title('Final Value Function (Policy Iteration)')\n",
        "        plt.xlabel('Capital')\n",
        "        plt.ylabel('Value (Probability of Winning)')\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "pi_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here you run your Policy Iteration\n",
        "gambler_pi  =  GamblersProblemPI(p_h = 0.4)\n",
        "pi_history, pi_policy, pi_values  =  gambler_pi.policy_iteration()\n",
        "\n",
        "gambler_pi.plot_policy_history(pi_history)\n",
        "gambler_pi.plot_value_function()"
      ],
      "metadata": {
        "id": "pi_run"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}